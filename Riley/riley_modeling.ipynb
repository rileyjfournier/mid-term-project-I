{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f164fe66-9c42-446c-8e7a-c2c14debc7b0",
   "metadata": {},
   "source": [
    "## ML Modeling\n",
    "Here is the main working notebook for all of my modeling.\n",
    "\n",
    "Open table of contents for easier navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cd2fe-cecc-4da8-9d1e-5eb1a871cbe8",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66365279-53e5-4b74-87ca-5d8cc05b4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import working libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "\n",
    "# scaling tools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# model selection tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50edf30b-492a-40c7-8ee9-4571261320ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in training data from csv\n",
    "df = pd.read_csv('../final_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce43f8a3-c58e-4966-9d5d-66ec0c2dcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X & y\n",
    "X = df.drop(columns=['arr_delay'])\n",
    "y = df['arr_delay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a489eb6-96ef-47b1-88b1-29d231c304eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23c1aa1-4145-45de-9a9c-8c5f4b6921b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variables\n",
    "encoder = ce.OrdinalEncoder(cols=['mkt_unique_carrier', 'tail_num', 'branded_code_share'])\n",
    "\n",
    "# fit onto training and testing DataFrames\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a83574-1d8b-4031-ac07-6adc045d8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 109718 entries, 95770 to 136767\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   mkt_carrier_fl_num       109718 non-null  int64  \n",
      " 1   origin_airport_id        109718 non-null  int64  \n",
      " 2   dest_airport_id          109718 non-null  int64  \n",
      " 3   crs_dep_time             109718 non-null  int64  \n",
      " 4   crs_arr_time             109718 non-null  int64  \n",
      " 5   crs_elapsed_time         109718 non-null  float64\n",
      " 6   distance                 109718 non-null  float64\n",
      " 7   mkt_unique_carrier       109718 non-null  int64  \n",
      " 8   tail_num                 109718 non-null  int64  \n",
      " 9   branded_code_share       109718 non-null  int64  \n",
      " 10  year                     109718 non-null  int64  \n",
      " 11  month                    109718 non-null  int64  \n",
      " 12  week                     109718 non-null  int64  \n",
      " 13  day                      109718 non-null  int64  \n",
      " 14  day_of_week              109718 non-null  int64  \n",
      " 15  mean_taxi_out/time       109718 non-null  float64\n",
      " 16  mean_taxi_in/time        109718 non-null  float64\n",
      " 17  mean_dep_delay/time      109718 non-null  float64\n",
      " 18  mean_arr_delay/time      109718 non-null  float64\n",
      " 19  mean_dep_delay/distance  109718 non-null  float64\n",
      " 20  mean_arr_delay/distance  109718 non-null  float64\n",
      " 21  mean_dep_delay/carrier   109718 non-null  float64\n",
      " 22  mean_arr_delay/carrier   109718 non-null  float64\n",
      " 23  avg_monthly_pas          109718 non-null  float64\n",
      "dtypes: float64(11), int64(13)\n",
      "memory usage: 20.9 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ab0bf-b254-496c-addc-5eb9d2dc2e00",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4b41c6-c676-4786-a974-7d2906fd0b47",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2745df8d-0040-47f9-a07d-e0dc97f52f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get cross validation scores\n",
    "def get_cv_scores(model):\n",
    "    scores = cross_val_score(model,\n",
    "                             X_train,\n",
    "                             y_train,\n",
    "                             cv=5,\n",
    "                             scoring='r2')\n",
    "    \n",
    "    print('CV Mean: ', np.mean(scores))\n",
    "    print('STD: ', np.std(scores))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2a11f3b-9769-4571-a3b9-df269ae9ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_scores(model, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    '''\n",
    "    This function takes in a model and train test split data,\n",
    "    fits the model, predicts on train and test data,\n",
    "    and prints RMSE and R^2 scores for training and test sets.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Must be an sklearn.linear_model with attached parenthesis(). \n",
    "                - ie. LinearRegression()\n",
    "    X_train\n",
    "    X_test\n",
    "    y_train\n",
    "    y_test\n",
    "            \n",
    "    Returns:\n",
    "    Prints RMSE and R^2 metric scores on y_train and y_test data sets\n",
    "    '''\n",
    "    \n",
    "    mdl = model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on train\n",
    "    y_pred_train = mdl.predict(X_train)\n",
    "\n",
    "    # predict on test\n",
    "    y_pred_test = mdl.predict(X_test)\n",
    "    \n",
    "    # print metric scores\n",
    "    print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "    print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "    print()\n",
    "    print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "    print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcaae78c-2d04-4beb-8a10-a42e059ce6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_regression_model(degree):\n",
    "    \n",
    "    \"Creates a polynomial regression model for the given degree\"\n",
    "  \n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    \n",
    "    # transforms the existing features to higher degree features.\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    \n",
    "    \n",
    "    # fit the transformed features to Linear Regression\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    \n",
    "    # predicting on training data-set\n",
    "    y_train_predicted = poly_model.predict(X_train_poly)\n",
    "    \n",
    "    \n",
    "    # predicting on test data-set\n",
    "    y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n",
    "    \n",
    "    \n",
    "    # evaluating the model on training dataset\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n",
    "    r2_train = r2_score(y_train, y_train_predicted)\n",
    "    \n",
    "    \n",
    "    # evaluating the model on test dataset\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n",
    "    r2_test = r2_score(y_test, y_test_predict)\n",
    "    \n",
    "    \n",
    "    print(\"The model performance for the training set\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "    print(\"R2 score of training set is {}\".format(r2_train))\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"The model performance for the test set\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "    print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce99d5-3209-4351-9830-2608b943af03",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6e612-c17c-43bd-984d-d72228a9dbbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Feature Selection / Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e48109-bfdc-47bc-9dc9-8510cc136bdf",
   "metadata": {},
   "source": [
    "We need to apply different selection techniques to find out which one will be the best for our problems.\n",
    "\n",
    "- Original Features vs. PCA conponents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c9451ac-f634-4324-817a-8368b92e54ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You must normalize the data before applying the fit method\n",
    "df_normalized=(X_train - X_train.mean()) / X_train.std()\n",
    "pca = PCA(n_components=X_train.shape[1])\n",
    "pca.fit(df_normalized)\n",
    "\n",
    "# Reformat and view results\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "columns=['PC%s' % _ for _ in range(len(df_normalized.columns))],\n",
    "index=X_train.columns)\n",
    "\n",
    "# plot pca components\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c388e9-14a5-4dd3-8337-a49e621ab880",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35189f88-c176-45b1-998d-3522cb1915d2",
   "metadata": {},
   "source": [
    "I will be using the following models.\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Elastic Net\n",
    "- Polynomial Regression\n",
    "- Random Forest\n",
    "- AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434290b-e4fb-4f0d-9b92-7ced864777fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regression Models Without Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935fce4c-2274-45c4-9c55-25d48f7538f7",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0ebf95-bb79-403c-86d5-f79c714f03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.7256304933709\n",
      "Test  -   R2: 0.009811203780386912\n",
      "\n",
      "Train - RMSE: 47.370105492022034\n",
      "Train -   R2: 0.010463486234568498\n"
     ]
    }
   ],
   "source": [
    "# using helper function\n",
    "prediction_scores(LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "384ff6d9-901c-4998-b5b6-67eb50adfea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.7256304933709\n",
      "Test  -   R2: 0.009811203780386912\n",
      "\n",
      "Train - RMSE: 47.370105492022034\n",
      "Train -   R2: 0.010463486234568498\n"
     ]
    }
   ],
   "source": [
    "# train LR model without helper function\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34e5c3-01c8-4387-88e9-fef280afca2f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae6e5b-ac92-4716-8041-43ebad12bbe8",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7de41625-49be-4d89-9b8d-136416c10f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.725630418400044\n",
      "Test  -   R2: 0.0098112067661833\n",
      "\n",
      "Train - RMSE: 47.37010549202303\n",
      "Train -   R2: 0.010463486234526975\n"
     ]
    }
   ],
   "source": [
    "# Train model with default alpha = 1\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98a84720-5165-4a48-b7e3-7b66549b3526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best Score:  0.009902146347977325\n",
      "Best Params:  {'alpha': 1000}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "\n",
    "# create model\n",
    "grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f97260c-ed3e-46c3-89c7-467c3fb34729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72555723829629\n",
      "Test  -   R2: 0.009814121241663809\n",
      "\n",
      "Train - RMSE: 47.37010643921397\n",
      "Train -   R2: 0.010463446661888143\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "ridge = Ridge(alpha=1000).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338f792-4ca3-4a50-83b5-4482c876d9e3",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3b328-d721-4651-914b-87d4dc5b3da1",
   "metadata": {},
   "source": [
    "##### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58697e6d-aecd-432b-850b-f619a12bd6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72290068722188\n",
      "Test  -   R2: 0.009919918310232645\n",
      "\n",
      "Train - RMSE: 47.378551027936325\n",
      "Train -   R2: 0.010110609200785103\n"
     ]
    }
   ],
   "source": [
    "# Train model with default parameters\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lasso.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lasso.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d637e892-07a1-4566-813c-78cdb77eb4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best Score:  0.009941882962259752\n",
      "Best Params:  {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "\n",
    "# create model\n",
    "grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "688dd719-e79c-46b2-b067-7777d538fafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.723746285231265\n",
      "Test  -   R2: 0.009886243007564244\n",
      "\n",
      "Train - RMSE: 47.37090680919174\n",
      "Train -   R2: 0.010430007765523275\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lasso.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lasso.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652a081-4a06-4255-b3a3-bce42f4bd2dd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ace4e4-835a-40d7-9d2b-73d700d63ef8",
   "metadata": {},
   "source": [
    "##### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a966b8a-5628-4ba1-bf43-f3feb8eed866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.721818502645846\n",
      "Test  -   R2: 0.009963014658955216\n",
      "\n",
      "Train - RMSE: 47.37576624371249\n",
      "Train -   R2: 0.010226971870618495\n"
     ]
    }
   ],
   "source": [
    "# train model with default parameters\n",
    "elastic_net = ElasticNet().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = elastic_net.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = elastic_net.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ff7e09e-1133-4084-9197-70b1c9eb9239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n",
      "Best Score:  0.009941882962259752\n",
      "Best Params:  {'alpha': 0.1, 'l1_ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1_ratio = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "# creat model\n",
    "grid = GridSearchCV(estimator=elastic_net, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ffbd917-9c60-45ae-941a-6b000a9b28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72243105899726\n",
      "Test  -   R2: 0.009938620652564678\n",
      "\n",
      "Train - RMSE: 47.37295492191873\n",
      "Train -   R2: 0.010344436479439612\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=1).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = elastic_net.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = elastic_net.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60010544-960a-4bc2-b9e8-eb433c25063a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7121a10e-a1f0-42b5-8c20-7eb0af585228",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Regression Models With Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa9eb578-9853-4951-8dc6-584b59606c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale training and test data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c8129-b842-4af5-a818-936b70ee9562",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e404c8d6-0a43-46d5-98f9-fb9d75c20ca5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.725629292117574\n",
      "Test  -   R2: 0.009811251621613448\n",
      "\n",
      "Train - RMSE: 47.37010549202203\n",
      "Train -   R2: 0.010463486234568609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.969e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.665e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.840e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.723e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.715e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.664e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.842e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.718e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.668e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.975e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.969e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.971e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.670e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.969e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.973e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.836e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.721e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.843e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.836e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.838e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.669e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.976e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.663e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.663e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.723e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# train LR model without helper function\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a433d-9cc6-4d63-8515-401001e60a6d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a32d11-645a-44f1-9e9c-2db479dd7dea",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c48bf4bc-c8a1-4387-9238-e98f5284379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72562896298813\n",
      "Test  -   R2: 0.00981126472955296\n",
      "\n",
      "Train - RMSE: 47.37010549206762\n",
      "Train -   R2: 0.010463486232663799\n"
     ]
    }
   ],
   "source": [
    "# Train model with default alpha = 1\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a702d9e3-435f-4fd0-a018-79015dd36815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best Score:  0.009939153849484428\n",
      "Best Params:  {'alpha': 1000}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "\n",
    "# create model\n",
    "grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4a1307f-453e-4788-a8cf-cc076e2339ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72538733017534\n",
      "Test  -   R2: 0.009820887996788152\n",
      "\n",
      "Train - RMSE: 47.370138194457624\n",
      "Train -   R2: 0.01046211996084212\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "ridge = Ridge(alpha=1000).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = ridge.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = ridge.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cf9cb-3737-481a-bed9-38be3a480f3e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a675424-9a16-4d15-b470-a29467e9566e",
   "metadata": {},
   "source": [
    "##### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a76f45e-21a5-453c-8d87-67775115737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.76062575899115\n",
      "Test  -   R2: 0.008416988642818701\n",
      "\n",
      "Train - RMSE: 47.41196623959991\n",
      "Train -   R2: 0.00871381562123874\n"
     ]
    }
   ],
   "source": [
    "# Train model with default parameters\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lasso.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lasso.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51f0da37-c0c0-4e1e-80db-f49209deeb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best Score:  0.009959448394709436\n",
      "Best Params:  {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "\n",
    "# create model\n",
    "grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "079a3908-3ef5-4d38-b6d3-a8b049273558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72243105899726\n",
      "Test  -   R2: 0.009938620652564678\n",
      "\n",
      "Train - RMSE: 47.37295492191873\n",
      "Train -   R2: 0.010344436479439612\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = lasso.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = lasso.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e42797-9ce0-48cc-b40f-1ace5ea3e299",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf25fe-7503-4670-831b-fffee8be4d37",
   "metadata": {},
   "source": [
    "##### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3404f7dd-d0fa-49b8-b295-0448f726bb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.76271822288391\n",
      "Test  -   R2: 0.008333593579325504\n",
      "\n",
      "Train - RMSE: 47.41626995251868\n",
      "Train -   R2: 0.008533843975931488\n"
     ]
    }
   ],
   "source": [
    "# train model with default parameters\n",
    "elastic_net = ElasticNet().fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = elastic_net.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = elastic_net.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66bb04a2-5f2e-4316-8bf5-2b7207836dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n",
      "Best Score:  0.009959448394709436\n",
      "Best Params:  {'alpha': 0.01, 'l1_ratio': 1}\n"
     ]
    }
   ],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "l1_ratio = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "# creat model\n",
    "grid = GridSearchCV(estimator=elastic_net, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# print best score and parameters\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0aad3c9-df1d-4e45-a63a-7e2b60e51b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.72243105899726\n",
      "Test  -   R2: 0.009938620652564678\n",
      "\n",
      "Train - RMSE: 47.37295492191873\n",
      "Train -   R2: 0.010344436479439612\n"
     ]
    }
   ],
   "source": [
    "# Train model with new alpha\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=1).fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = elastic_net.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = elastic_net.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac60981-a038-4e6a-8ab7-490d1dc12897",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c6a9c-9c2b-4e62-9589-17e1de523bc5",
   "metadata": {},
   "source": [
    "#### Polynomial Regression With Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484a7a95-4371-474d-8d07-355fd03a4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for the training set\n",
      "-------------------------------------------\n",
      "RMSE of training set is 47.150909319039535\n",
      "R2 score of training set is 0.019600083113504585\n",
      "\n",
      "\n",
      "The model performance for the test set\n",
      "-------------------------------------------\n",
      "RMSE of test set is 191880.29149298512\n",
      "R2 score of test set is -14744095.011314606\n"
     ]
    }
   ],
   "source": [
    "create_polynomial_regression_model(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dcea551-961c-42b7-9e61-52658e3a89b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for the training set\n",
      "-------------------------------------------\n",
      "RMSE of training set is 46.327855162599576\n",
      "R2 score of training set is 0.05352857037302883\n",
      "\n",
      "\n",
      "The model performance for the test set\n",
      "-------------------------------------------\n",
      "RMSE of test set is 80005990.53295365\n",
      "R2 score of test set is -2563317627587.0947\n"
     ]
    }
   ],
   "source": [
    "create_polynomial_regression_model(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd87690-b43f-436b-829a-054150133dff",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a7e36-5a12-49ea-a857-0ea2f35a0216",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1ed56-ebc9-4900-b2a8-e4077417aef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Random Forest Grid Search without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58cfde62-120f-4f2b-9541-8277462683e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.969e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.819e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.937e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.723e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.924e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.836e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.976e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.750e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.764e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.663e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.877e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.807e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.016e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.664e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.703e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.938e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.843e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.757e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.016e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.970e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.670e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.763e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.821e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "### RUN THE FOLLOWING TO GET VALUES BEFORE SCALING\n",
    "\n",
    "# bring in training data from csv\n",
    "df = pd.read_csv('../final_training.csv')\n",
    "\n",
    "# set X & y\n",
    "X = df.drop(columns=['arr_delay'])\n",
    "y = df['arr_delay']\n",
    "\n",
    "# split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "# encode categorical variables\n",
    "encoder = ce.OrdinalEncoder(cols=['mkt_unique_carrier', 'tail_num', 'branded_code_share'])\n",
    "\n",
    "# fit onto training and testing DataFrames\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08f30c7b-8cb9-499e-924b-3d19911af414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "rfc = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68167f74-462e-4b65-b3dd-381875f6ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters for grid search\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "349dec7d-e0cb-4983-9ef2-d8fe2b82b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [4, 5, 6, 7, 8],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2'],\n",
       "                         'n_estimators': [200, 500]})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit GridSearch\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94792ef5-bdc2-46ea-aef5-bd26f720f1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 7,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the best parameters\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28e94103-6e08-4412-89ed-7d534198d6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 52.287670120114996\n",
      "Test  -   R2: -0.09485345287142621\n",
      "\n",
      "Train - RMSE: 49.98695850627074\n",
      "Train -   R2: -0.10188569669692105\n"
     ]
    }
   ],
   "source": [
    "# create new model with optimized parameters\n",
    "rfc1 = RandomForestClassifier(random_state=42, \n",
    "                              max_features='auto', \n",
    "                              n_estimators= 200, \n",
    "                              max_depth=7, \n",
    "                              criterion='gini', \n",
    "                              n_jobs=-1)\n",
    "\n",
    "# fit new model\n",
    "rfc1.fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = rfc1.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = rfc1.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c1fb1-d480-42ea-b778-393c03a261a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Random Forest With Scaling, No Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1047c65-bcab-4c0b-ae66-4dfab61a103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd781d48-3640-4a01-b033-efafb350d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model with optimized parameters\n",
    "rfc1 = RandomForestClassifier(random_state=42, \n",
    "                              max_features='auto', \n",
    "                              n_estimators= 200, \n",
    "                              max_depth=7, \n",
    "                              criterion='gini', \n",
    "                              n_jobs=-1)\n",
    "\n",
    "# fit new model\n",
    "rfc1.fit(X_train, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = rfc1.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = rfc1.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f826f53-39e0-4f2e-af25-38ea43d12162",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdae148-301d-4155-b371-5ad08bb056ba",
   "metadata": {},
   "source": [
    "##### Random Forest with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f287c1d9-3c7a-466b-baed-0422d57fae32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.969e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.819e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.005e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.010e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.937e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.723e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.924e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.836e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.976e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.750e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.764e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.663e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.877e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.807e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.016e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.664e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.703e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.006e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.938e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.716e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.837e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.843e+07, tolerance: 1.988e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.757e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.016e+08, tolerance: 2.032e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.007e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.970e+07, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.670e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+08, tolerance: 2.015e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.763e+07, tolerance: 1.953e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.821e+07, tolerance: 1.964e+04 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "### RUN THE FOLLOWING TO GET VALUES BEFORE SCALING\n",
    "\n",
    "# bring in training data from csv\n",
    "df = pd.read_csv('../final_training.csv')\n",
    "\n",
    "# set X & y\n",
    "X = df.drop(columns=['arr_delay'])\n",
    "y = df['arr_delay']\n",
    "\n",
    "# split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "# encode categorical variables\n",
    "encoder = ce.OrdinalEncoder(cols=['mkt_unique_carrier', 'tail_num', 'branded_code_share'])\n",
    "\n",
    "# fit onto training and testing DataFrames\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "748eb3e9-0455-4ec9-9caa-12b48fb82fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(f_regression, k=12)\n",
    "X_train_selected = skb.fit_transform(X_train, y_train)\n",
    "X_test_selected = skb.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcec88f1-bdb4-47d5-9c9a-208402dd66f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 51.388794500770636\n",
      "Test  -   R2: -0.057533834719314836\n",
      "\n",
      "Train - RMSE: 50.06038780262382\n",
      "Train -   R2: -0.10512534645523464\n"
     ]
    }
   ],
   "source": [
    "# create new model with optimized parameters\n",
    "rfc1 = RandomForestClassifier(random_state=42, \n",
    "                              max_features='auto', \n",
    "                              n_estimators= 200, \n",
    "                              max_depth=7, \n",
    "                              criterion='gini', \n",
    "                              n_jobs=-1)\n",
    "\n",
    "# fit new model\n",
    "rfc1.fit(X_train_selected, y_train)\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = rfc1.predict(X_train_selected)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = rfc1.predict(X_test_selected)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b78fe-f2af-468c-8ecc-756ee87c07ee",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed29c2-e2eb-440a-9ae1-f60501172303",
   "metadata": {},
   "source": [
    "#### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "92d609f4-ea17-453c-bc5d-76dc0f47d0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### RUN THE FOLLOWING TO GET VALUES BEFORE SCALING\n",
    "\n",
    "# bring in training data from csv\n",
    "df = pd.read_csv('../final_training.csv')\n",
    "\n",
    "# set X & y\n",
    "X = df.drop(columns=['arr_delay'])\n",
    "y = df['arr_delay']\n",
    "\n",
    "# split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 101)\n",
    "\n",
    "# encode categorical variables\n",
    "encoder = ce.OrdinalEncoder(cols=['mkt_unique_carrier', 'tail_num', 'branded_code_share'])\n",
    "\n",
    "# fit onto training and testing DataFrames\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c1678150-0812-42f7-bc64-387a6674ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c07e1615-4587-466a-b638-d4867fb7555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_grid = { \n",
    "    'n_estimators': [500, 600, 700],\n",
    "    'learning_rate': [0.00001, 0.00001],\n",
    "    'random_state' : [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "95db7a67-cfe0-40a5-b1c4-87d4ba0356fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=AdaBoostRegressor(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [1e-05, 1e-05],\n",
       "                         'n_estimators': [500, 600, 700], 'random_state': [1]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearch with AdaBoostRegressor\n",
    "CV_ada = GridSearchCV(estimator=ada, \n",
    "                      param_grid=search_grid, \n",
    "                      scoring='r2', \n",
    "                      n_jobs=-1)\n",
    "\n",
    "CV_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "80acc081-2fc1-45d2-97eb-12935d1c9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_ada.best_score_: 0.014649709015664824\n",
      "CV_ada.best_params_: {'learning_rate': 1e-05, 'n_estimators': 500, 'random_state': 1}\n"
     ]
    }
   ],
   "source": [
    "print('CV_ada.best_score_:', CV_ada.best_score_)\n",
    "print('CV_ada.best_params_:', CV_ada.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "253e7319-2ae5-431b-910f-c09b6738cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - RMSE: 49.59527971464768\n",
      "Test  -   R2: 0.014995761718137879\n",
      "\n",
      "Train - RMSE: 47.24604944724254\n",
      "Train -   R2: 0.015639630189483666\n"
     ]
    }
   ],
   "source": [
    "# Train model with new parameters\n",
    "ada = AdaBoostRegressor(learning_rate=0.00001, \n",
    "                        n_estimators=500, \n",
    "                        random_state=1).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predict on train\n",
    "y_pred_train = ada.predict(X_train)\n",
    "\n",
    "# predict on test\n",
    "y_pred_test = ada.predict(X_test)\n",
    "\n",
    "# print metric scores\n",
    "print('Test  - RMSE:', metrics.mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "print('Test  -   R2:', metrics.r2_score(y_test, y_pred_test))\n",
    "print()\n",
    "print('Train - RMSE:', metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "print('Train -   R2:', metrics.r2_score(y_train, y_pred_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
